#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
×¡×§×¨×™×¤×˜ ×××™×ª×™ ×œ×’×™×¨×•×“ ××ª×¨×™× - ×¢× ×©×¨×ª Flask API
×××¤×©×¨ ×œ××¤×œ×™×§×¦×™×™×ª HTML ×œ×’×¨×“ ×ª×•×›×Ÿ ×××™×ª×™
"""

from flask import Flask, request, jsonify, render_template_string, send_from_directory
from flask_cors import CORS
import requests
from bs4 import BeautifulSoup
import json
import time
import urllib.parse
import logging
from datetime import datetime
import os
import threading
from urllib.robotparser import RobotFileParser

# ×”×’×“×¨×ª ×œ×•×’×™×
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # ×××¤×©×¨ CORS ×œ×›×œ ×”×“×•××™×™× ×™×

class RealWebScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'he-IL,he;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })
    
    def scrape_url(self, url, settings=None):
        """×’×™×¨×•×“ URL ×××™×ª×™"""
        if not settings:
            settings = {
                'delay': 1,
                'maxLinks': 20,
                'maxImages': 10,
                'textLength': 1000,
                'respectRobots': True
            }
        
        try:
            logger.info(f"××ª×—×™×œ ×’×™×¨×•×“ ×××™×ª×™: {url}")
            
            # ×‘×“×™×§×ª robots.txt ×× × ×“×¨×©
            if settings.get('respectRobots', True):
                if not self._check_robots_txt(url):
                    return {
                        'url': url,
                        'error': 'Access denied by robots.txt',
                        'status': 'error',
                        'scrapedAt': datetime.now().isoformat()
                    }
            
            # ×”×©×”×™×”
            time.sleep(settings.get('delay', 1))
            
            # ×‘×™×¦×•×¢ ×”×‘×§×©×”
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            response.encoding = response.apparent_encoding or 'utf-8'
            
            # ×¤×™×¨×•×§ HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ×—×™×œ×•×¥ × ×ª×•× ×™× ×××™×ª×™×™× ×›×•×œ×œ HTML ××œ×
            result = {
                'url': url,
                'title': self._extract_title(soup),
                'description': self._extract_meta_description(soup),
                'keywords': self._extract_meta_keywords(soup),
                'headings': self._extract_headings(soup),
                'links': self._extract_links(soup, url, settings.get('maxLinks', 20)),
                'images': self._extract_images(soup, url, settings.get('maxImages', 10)),
                'textContent': self._extract_text_content(soup, settings.get('textLength', 1000)),
                'responseTime': int(response.elapsed.total_seconds() * 1000),
                'pageSize': len(response.content),
                'statusCode': response.status_code,
                'contentType': response.headers.get('content-type', ''),
                'lastModified': response.headers.get('last-modified', ''),
                'scrapedAt': datetime.now().isoformat(),
                'status': 'success',
                'fullHtml': response.text
            }
            
            logger.info(f"×’×™×¨×•×“ ×”×•×©×œ× ×‘×”×¦×œ×—×”: {url}")
            return result
            
        except requests.exceptions.RequestException as e:
            logger.error(f"×©×’×™××” ×‘×’×™×¨×•×“ {url}: {e}")
            return {
                'url': url,
                'error': str(e),
                'status': 'error',
                'scrapedAt': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"×©×’×™××” ×›×œ×œ×™×ª ×‘×’×™×¨×•×“ {url}: {e}")
            return {
                'url': url,
                'error': f"Internal error: {str(e)}",
                'status': 'error',
                'scrapedAt': datetime.now().isoformat()
            }
    
    def _check_robots_txt(self, url):
        """×‘×“×™×§×ª robots.txt"""
        try:
            parsed_url = urllib.parse.urlparse(url)
            robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
            
            rp = RobotFileParser()
            rp.set_url(robots_url)
            rp.read()
            
            user_agent = self.session.headers.get('User-Agent', '*')
            return rp.can_fetch(user_agent, url)
        except:
            return True  # ×‘××§×¨×” ×©×œ ×©×’×™××”, × ××¤×©×¨ ×’×™×¨×•×“
    
    def _extract_title(self, soup):
        """×—×™×œ×•×¥ ×›×•×ª×¨×ª ×××™×ª×™×ª"""
        title = soup.find('title')
        if title:
            return title.get_text().strip()
        
        # × ×¡×™×•×Ÿ × ×•×¡×£ - Open Graph title
        og_title = soup.find('meta', property='og:title')
        if og_title:
            return og_title.get('content', '').strip()
        
        return "×œ×œ× ×›×•×ª×¨×ª"
    
    def _extract_meta_description(self, soup):
        """×—×™×œ×•×¥ ×ª×™××•×¨ meta ×××™×ª×™"""
        # ×ª×™××•×¨ ×¨×’×™×œ
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc:
            return meta_desc.get('content', '').strip()
        
        # Open Graph description
        og_desc = soup.find('meta', property='og:description')
        if og_desc:
            return og_desc.get('content', '').strip()
        
        return ""
    
    def _extract_meta_keywords(self, soup):
        """×—×™×œ×•×¥ ××™×œ×•×ª ××¤×ª×— meta"""
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        return meta_keywords.get('content', '').strip() if meta_keywords else ""
    
    def _extract_headings(self, soup):
        """×—×™×œ×•×¥ ×›×•×ª×¨×•×ª ×××™×ª×™×•×ª"""
        headings = {}
        for i in range(1, 7):
            tags = soup.find_all(f'h{i}')
            if tags:
                headings[f'h{i}'] = [tag.get_text().strip() for tag in tags if tag.get_text().strip()]
        return headings
    
    def _extract_links(self, soup, base_url, max_links):
        """×—×™×œ×•×¥ ×§×™×©×•×¨×™× ×××™×ª×™×™×"""
        links = []
        base_domain = urllib.parse.urlparse(base_url).netloc
        
        for link in soup.find_all('a', href=True):
            if len(links) >= max_links:
                break
                
            href = link['href'].strip()
            if not href or href.startswith('#') or href.startswith('javascript:'):
                continue
            
            # ×”××¨×” ×œ×›×ª×•×‘×ª ××•×—×œ×˜×ª
            absolute_url = urllib.parse.urljoin(base_url, href)
            link_domain = urllib.parse.urlparse(absolute_url).netloc
            
            # ×—×™×œ×•×¥ ×˜×§×¡×˜ ×”×§×™×©×•×¨
            link_text = link.get_text().strip()
            if not link_text:
                # × ×¡×™×•×Ÿ ×œ×—×œ×¥ ×-title ××• aria-label
                link_text = link.get('title', '').strip() or link.get('aria-label', '').strip() or '×§×™×©×•×¨ ×œ×œ× ×˜×§×¡×˜'
            
            links.append({
                'text': link_text[:100],  # ××’×‘×™×œ ××•×¨×š ×˜×§×¡×˜
                'url': absolute_url,
                'isInternal': link_domain == base_domain,
                'title': link.get('title', '').strip()
            })
        
        return links
    
    def _extract_images(self, soup, base_url, max_images):
        """×—×™×œ×•×¥ ×ª××•× ×•×ª ×××™×ª×™×•×ª"""
        images = []
        
        for img in soup.find_all('img'):
            if len(images) >= max_images:
                break
                
            src = img.get('src', '').strip()
            if not src:
                # × ×¡×™×•×Ÿ ×œ×—×œ×¥ ×-data-src (lazy loading)
                src = img.get('data-src', '').strip()
            
            if not src:
                continue
            
            # ×”××¨×” ×œ×›×ª×•×‘×ª ××•×—×œ×˜×ª
            absolute_url = urllib.parse.urljoin(base_url, src)
            
            images.append({
                'src': absolute_url,
                'alt': img.get('alt', '').strip(),
                'title': img.get('title', '').strip(),
                'width': img.get('width', ''),
                'height': img.get('height', ''),
                'loading': img.get('loading', ''),
                'class': ' '.join(img.get('class', []))
            })
        
        return images
    
    def _extract_text_content(self, soup, max_length):
        """×—×™×œ×•×¥ ×ª×•×›×Ÿ ×˜×§×¡×˜ ×××™×ª×™"""
        # ×”×¡×¨×ª ××œ×× ×˜×™× ×œ× ×¨×œ×•×•× ×˜×™×™×
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'noscript']):
            element.decompose()
        
        # ×—×™×¤×•×© ×ª×•×›×Ÿ ×¢×™×§×¨×™
        main_content = None
        
        # × ×¡×™×•×Ÿ ×œ××¦×•× ×ª×•×›×Ÿ ×¢×™×§×¨×™ ×œ×¤×™ ×ª×’×™×•×ª ×•××—×œ×§×•×ª × ×¤×•×¦×•×ª
        for selector in ['main', 'article', '.content', '.main-content', '.post-content', '#content', '.entry-content']:
            main_content = soup.select_one(selector)
            if main_content:
                break
        
        # ×× ×œ× × ××¦× ×ª×•×›×Ÿ ×¢×™×§×¨×™, × ×©×ª××© ×‘×›×œ ×”×’×•×£
        if not main_content:
            main_content = soup.find('body') or soup
        
        # ×—×™×œ×•×¥ ×˜×§×¡×˜ × ×§×™
        text = main_content.get_text(separator=' ', strip=True)
        
        # × ×™×§×•×™ ×˜×§×¡×˜ ××ª×§×“×
        lines = [line.strip() for line in text.splitlines()]
        lines = [line for line in lines if line and len(line) > 3]  # ×”×¡×¨×ª ×©×•×¨×•×ª ×§×¦×¨×•×ª ××“×™
        
        # ××™×—×•×“ ×©×•×¨×•×ª
        text = ' '.join(lines)
        
        # × ×™×§×•×™ ×¨×•×•×—×™× ××™×•×ª×¨×™×
        import re
        text = re.sub(r'\s+', ' ', text)
        
        # ×”×—×–×¨×ª ×˜×§×¡×˜ ×—×ª×•×š ×œ×¤×™ ×”×’×‘×œ×”
        return text[:max_length] if text else ""

# ×™×¦×™×¨×ª instance ×’×œ×•×‘×œ×™
scraper = RealWebScraper()

@app.route('/')
def home():
    """×“×£ ×‘×™×ª ×¢× ×××©×§ ×”×’×™×¨×•×“"""
    return send_from_directory('.', 'scraper-app.html')

@app.route('/<path:filename>')
def static_files(filename):
    """×§×‘×¦×™× ×¡×˜×˜×™×™×"""
    return send_from_directory('.', filename)

@app.route('/api/scrape', methods=['POST'])
def api_scrape():
    """API ×œ×’×™×¨×•×“ ××ª×¨ ×™×—×™×“"""
    try:
        data = request.json
        url = data.get('url')
        settings = data.get('settings', {})
        
        if not url:
            return jsonify({'error': 'URL is required'}), 400
        
        result = scraper.scrape_url(url, settings)
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘-API: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/scrape_multiple', methods=['POST'])
def api_scrape_multiple():
    """API ×œ×’×™×¨×•×“ ××¨×•×‘×”"""
    try:
        data = request.json
        urls = data.get('urls', [])
        settings = data.get('settings', {})
        
        if not urls:
            return jsonify({'error': 'URLs are required'}), 400
        
        results = []
        for url in urls:
            result = scraper.scrape_url(url, settings)
            results.append(result)
        
        return jsonify({'results': results})
        
    except Exception as e:
        logger.error(f"×©×’×™××” ×‘-API ××¨×•×‘×”: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/test')
def api_test():
    """×‘×“×™×§×ª ×—×™×‘×•×¨ API"""
    return jsonify({
        'status': 'OK',
        'message': 'Real Web Scraper API is running',
        'timestamp': datetime.now().isoformat()
    })

if __name__ == '__main__':
    print("ğŸ•·ï¸ ××¤×¢×™×œ ×©×¨×ª ×’×™×¨×•×“ ××ª×¨×™× ×××™×ª×™...")
    print("ğŸ“¡ ×”×©×¨×ª ×™×”×™×” ×–××™×Ÿ ×‘×›×ª×•×‘×ª: http://localhost:5000")
    print("ğŸŒ ×”××¤×œ×™×§×¦×™×” ×ª×”×™×” ×–××™× ×” ×‘: http://localhost:5000")
    print("âš¡ API endpoint: http://localhost:5000/api/scrape")
    print("ğŸ”„ ×œ×¢×¦×™×¨×”: Ctrl+C")
    
    app.run(host='0.0.0.0', port=5000, debug=False)